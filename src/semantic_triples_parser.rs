// ---------------------------------------------------------
// This file was generated by parol.
// It is not intended for manual editing and changes will be
// lost after next build.
// ---------------------------------------------------------

use parol_runtime::once_cell::sync::Lazy;
#[allow(unused_imports)]
use parol_runtime::parser::{LLKParser, LookaheadDFA, ParseTreeType, ParseType, Production, Trans};
use parol_runtime::{ParolError, ParseTree, TerminalIndex};
use parol_runtime::{TokenStream, Tokenizer};
use std::path::Path;

use crate::semantic_triples_grammar::SemanticTriplesGrammar;
use crate::semantic_triples_grammar_trait::SemanticTriplesGrammarAuto;

use parol_runtime::lexer::tokenizer::{
    ERROR_TOKEN, NEW_LINE_TOKEN, UNMATCHABLE_TOKEN, WHITESPACE_TOKEN,
};

pub const TERMINALS: &[&str; 9] = &[
    /* 0 */ UNMATCHABLE_TOKEN,
    /* 1 */ UNMATCHABLE_TOKEN,
    /* 2 */ UNMATCHABLE_TOKEN,
    /* 3 */ UNMATCHABLE_TOKEN,
    /* 4 */ UNMATCHABLE_TOKEN,
    /* 5 */ r"[a-zA-Z_][a-zA-Z0-9_]*",
    /* 6 */ r"\-\-",
    /* 7 */ r"\->",
    /* 8 */ ERROR_TOKEN,
];

pub const TERMINAL_NAMES: &[&str; 9] = &[
    /* 0 */ "EndOfInput",
    /* 1 */ "Newline",
    /* 2 */ "Whitespace",
    /* 3 */ "LineComment",
    /* 4 */ "BlockComment",
    /* 5 */ "Id",
    /* 6 */ "MinusMinus",
    /* 7 */ "MinusGT",
    /* 8 */ "Error",
];

/* SCANNER_0: "INITIAL" */
const SCANNER_0: (&[&str; 5], &[TerminalIndex; 3]) = (
    &[
        /* 0 */ UNMATCHABLE_TOKEN,
        /* 1 */ NEW_LINE_TOKEN,
        /* 2 */ WHITESPACE_TOKEN,
        /* 3 */ r"(//.*(\r\n|\r|\n|$))",
        /* 4 */ UNMATCHABLE_TOKEN,
    ],
    &[
        5, /* Id */
        6, /* MinusMinus */
        7, /* MinusGT */
    ],
);

const MAX_K: usize = 1;

pub const NON_TERMINALS: &[&str; 5] = &[
    /* 0 */ "Id",
    /* 1 */ "Predicate",
    /* 2 */ "SemanticTriple",
    /* 3 */ "SemanticTriples",
    /* 4 */ "SemanticTriplesList",
];

pub const LOOKAHEAD_AUTOMATA: &[LookaheadDFA; 5] = &[
    /* 0 - "Id" */
    LookaheadDFA {
        prod0: 4,
        transitions: &[],
        k: 0,
    },
    /* 1 - "Predicate" */
    LookaheadDFA {
        prod0: 5,
        transitions: &[],
        k: 0,
    },
    /* 2 - "SemanticTriple" */
    LookaheadDFA {
        prod0: 3,
        transitions: &[],
        k: 0,
    },
    /* 3 - "SemanticTriples" */
    LookaheadDFA {
        prod0: 0,
        transitions: &[],
        k: 0,
    },
    /* 4 - "SemanticTriplesList" */
    LookaheadDFA {
        prod0: -1,
        transitions: &[Trans(0, 0, 2, 2), Trans(0, 5, 1, 1)],
        k: 1,
    },
];

pub const PRODUCTIONS: &[Production; 6] = &[
    // 0 - SemanticTriples: SemanticTriplesList /* Vec */;
    Production {
        lhs: 3,
        production: &[ParseType::N(4)],
    },
    // 1 - SemanticTriplesList: SemanticTriple SemanticTriplesList;
    Production {
        lhs: 4,
        production: &[ParseType::N(4), ParseType::N(2)],
    },
    // 2 - SemanticTriplesList: ;
    Production {
        lhs: 4,
        production: &[],
    },
    // 3 - SemanticTriple: Id Predicate Id;
    Production {
        lhs: 2,
        production: &[ParseType::N(0), ParseType::N(1), ParseType::N(0)],
    },
    // 4 - Id: /[a-zA-Z_][a-zA-Z0-9_]*/;
    Production {
        lhs: 0,
        production: &[ParseType::T(5)],
    },
    // 5 - Predicate: '--'^ /* Clipped */ Id '->'^ /* Clipped */;
    Production {
        lhs: 1,
        production: &[ParseType::T(7), ParseType::N(0), ParseType::T(6)],
    },
];

static TOKENIZERS: Lazy<Vec<(&'static str, Tokenizer)>> = Lazy::new(|| {
    vec![(
        "INITIAL",
        Tokenizer::build(TERMINALS, SCANNER_0.0, SCANNER_0.1).unwrap(),
    )]
});

pub fn parse<'t, T>(
    input: &'t str,
    file_name: T,
    user_actions: &mut SemanticTriplesGrammar<'t>,
) -> Result<ParseTree<'t>, ParolError>
where
    T: AsRef<Path>,
{
    let mut llk_parser = LLKParser::new(
        3,
        LOOKAHEAD_AUTOMATA,
        PRODUCTIONS,
        TERMINAL_NAMES,
        NON_TERMINALS,
    );
    llk_parser.trim_parse_tree();
    // Initialize wrapper
    let mut user_actions = SemanticTriplesGrammarAuto::new(user_actions);

    llk_parser.parse(
        TokenStream::new(input, file_name, &TOKENIZERS, MAX_K).unwrap(),
        &mut user_actions,
    )
}
